diff --git a/python/sglang/srt/distributed/communication_op.py b/python/sglang/srt/distributed/communication_op.py
index 95600ed..4d1b9c7 100644
--- a/python/sglang/srt/distributed/communication_op.py
+++ b/python/sglang/srt/distributed/communication_op.py
@@ -33,3 +33,9 @@ def broadcast_tensor_dict(
     if not torch.distributed.is_initialized():
         return tensor_dict
     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)
+
+def tensor_model_parallel_tree_all_reduce(input_: torch.Tensor) -> torch.Tensor:
+    """Tree all-reduce the input tensor across model parallel group."""
+    from sglang.srt.tp_invariant_ops import tree_all_reduce_sum
+
+    return tree_all_reduce_sum(input_, device_group=get_tp_group().device_group)
\ No newline at end of file
diff --git a/python/sglang/srt/layers/communicator.py b/python/sglang/srt/layers/communicator.py
index 15df851..56b664a 100644
--- a/python/sglang/srt/layers/communicator.py
+++ b/python/sglang/srt/layers/communicator.py
@@ -25,6 +25,7 @@ from sglang.srt.distributed import (
     get_tensor_model_parallel_world_size,
     get_tp_group,
     tensor_model_parallel_all_reduce,
+    tensor_model_parallel_tree_all_reduce, 
 )
 from sglang.srt.distributed.device_communicators.pynccl_allocator import (
     use_symmetric_memory,
@@ -38,6 +39,7 @@ from sglang.srt.layers.dp_attention import (
     attn_tp_reduce_scatter_tensor,
     dp_gather_partial,
     dp_reduce_scatter_tensor,
+    dp_gather_replicate,
     dp_scatter,
     get_attention_dp_size,
     get_attention_tp_rank,
@@ -806,7 +808,10 @@ class CommunicateWithAllReduceAndLayerNormFn:
                     hidden_states, residual
                 )
             else:
-                hidden_states = tensor_model_parallel_all_reduce(hidden_states)
+                if get_global_server_args().rl_on_policy_target == "fsdp_tp" or get_global_server_args().rl_on_policy_target == "fsdp":
+                    hidden_states = tensor_model_parallel_tree_all_reduce(hidden_states)
+                else:
+                    hidden_states = tensor_model_parallel_all_reduce(hidden_states)
                 if _is_npu and context.cache is not None:
                     _ = prepare_weight_cache(hidden_states, context.cache)
                 hidden_states, residual = layernorm(hidden_states, residual)
diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
index 14d6923..9252b29 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
@@ -8,7 +8,8 @@ from __future__ import annotations
 import functools
 import os
 from typing import TYPE_CHECKING, List, Optional
-
+from sglang.srt.tp_invariant_ops.tp_invariant_ops import moe_sum_tree_reduce
+from sglang.srt.server_args import get_global_server_args
 import torch
 import torch.nn.functional as F
 import triton.language as tl
@@ -321,6 +322,21 @@ def fused_experts_impl(
     gemm1_limit: Optional[float] = None,
     filter_expert: bool = True,
 ):
+    import logging
+    import os
+    logger = logging.getLogger(__name__)
+    logger.info(f"fused_experts_impl")
+    logger.info(f"hidden_states.shape: {hidden_states.shape}, dtype: {hidden_states.dtype}")
+    if hidden_states.shape[0] > 91:
+        logger.info(f"hidden_states[91, :5]: {hidden_states[91, :5].tolist()}")
+        print(f"hidden_states[91, :5]: {hidden_states[91, :5].tolist()}")
+    else:
+        logger.info(f"hidden_states[0, :5]: {hidden_states[0, :5].tolist()}")
+        print(f"hidden_states[0, :5]: {hidden_states[0, :5].tolist()}")
+    logger.info(f"w1.shape: {w1.shape}, dtype: {w1.dtype}")
+    logger.info(f"w2.shape: {w2.shape}, dtype: {w2.dtype}")
+    logger.info(f"topk_weights.shape: {topk_weights.shape}, dtype: {topk_weights.dtype}")
+    logger.info(f"topk_ids.shape: {topk_ids.shape}, dtype: {topk_ids.dtype}")
     padded_size = padding_size
     if not (use_fp8_w8a8 or use_int8_w8a8) or block_shape is not None or _use_aiter:
         padded_size = 0
@@ -559,8 +575,7 @@ def fused_experts_impl(
         )
 
         if routed_scaling_factor is None:
-            routed_scaling_factor = 1.0
-
+            routed_scaling_factor = 1.0        
         if no_combine:
             pass
         elif _is_cuda:
@@ -572,6 +587,14 @@ def fused_experts_impl(
                     intermediate_cache3[:, 1],
                     out=out_hidden_states[begin_chunk_idx:end_chunk_idx],
                 ).squeeze(dim=1)
+            elif get_global_server_args().rl_on_policy_target and get_global_server_args().rl_on_policy_target == "fsdp_tp" or get_global_server_args().rl_on_policy_target == "fsdp":
+                moe_sum_tree_reduce(
+                    intermediate_cache3.view(*intermediate_cache3.shape),
+                    out_hidden_states[begin_chunk_idx:end_chunk_idx],
+                    curr_topk_ids,
+                    routed_scaling_factor,
+                    E,
+                )
             else:
                 # According to micro benchmark results, torch.compile can get better performance for small token.
                 if (
diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
index 8394635..08122bd 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
@@ -952,6 +952,15 @@ class FusedMoE(torch.nn.Module):
 
         if self.reduce_results and (self.moe_tp_size > 1 or self.moe_ep_size > 1):
             final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
+         # Debug: compare expert output (SGLang side) - only print for layer 0
+        import os
+        if os.environ.get("DEBUG_SGLANG_EP_MAPPING", "0") == "1" and self.layer_id == 0:
+            import logging
+            _logger = logging.getLogger(__name__)
+            _logger.info(f"[layer.py][SGLang Expert Output][Layer {self.layer_id}] output.shape: {final_hidden_states.shape}, dtype: {final_hidden_states.dtype}")
+            _logger.info(f"[layer.py][SGLang Expert Output][Layer {self.layer_id}] output[0, :5]: {final_hidden_states[0, :5].tolist()}")
+            _logger.info(f"[layer.py][SGLang Expert Output][Layer {self.layer_id}] output.norm(): {final_hidden_states.norm().item():.6f}")
+            _logger.info(f"[layer.py][SGLang Expert Output][Layer {self.layer_id}] output.sum(): {final_hidden_states.sum().item():.6f}")
 
         return final_hidden_states
 
diff --git a/python/sglang/srt/layers/moe/token_dispatcher/standard.py b/python/sglang/srt/layers/moe/token_dispatcher/standard.py
index 2c959c7..78ad6bb 100644
--- a/python/sglang/srt/layers/moe/token_dispatcher/standard.py
+++ b/python/sglang/srt/layers/moe/token_dispatcher/standard.py
@@ -32,6 +32,9 @@ from sglang.srt.layers.moe.utils import (
 )
 from sglang.srt.utils.common import get_bool_env_var, is_hip, is_sm120_supported
 
+import logging
+logger = logging.getLogger(__name__)
+
 _is_hip = is_hip()
 _use_aiter = get_bool_env_var("SGLANG_USE_AITER") and _is_hip
 
@@ -93,11 +96,12 @@ class StandardDispatcher(BaseDispatcher):
         )
         self.moe_ep_rank = get_moe_expert_parallel_rank()
         self.local_expert_mapping = None
+        self.layer_id = moe_runner_config.layer_id  # Store layer_id from config
 
     def dispatch(
         self, hidden_states: torch.Tensor, topk_output: TopKOutput
     ) -> StandardDispatchOutput:
-
+        import os
         if should_use_flashinfer_cutlass_moe_fp4_allgather():
             # all-gather fp4 hidden states
             from flashinfer import nvfp4_block_scale_interleave
@@ -167,10 +171,47 @@ class StandardDispatcher(BaseDispatcher):
                     )
 
         if self.local_expert_mapping is not None and not _use_aiter:
+            # Debug print for EP token mapping (only print for layer 0)
+            should_print = (
+                os.environ.get("DEBUG_SGLANG_EP_MAPPING", "0") == "1"
+                and self.layer_id == 0
+            )
+            
+            if should_print:
+                topk_ids_before = topk_output.topk_ids
+                num_tokens = topk_ids_before.shape[0]
+                topk = topk_ids_before.shape[1]
+                rank = self.moe_ep_rank
+                local_start = rank * self.num_local_routed_experts
+                local_end = local_start + self.num_local_routed_experts
+                
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] EP config: ep_size={self.moe_ep_size}, ep_rank={rank}")
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] num_experts={self.num_experts}, num_local_experts={self.num_local_routed_experts}")
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] num_tokens={num_tokens}, topk={topk}")
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] Local expert range: [{local_start}, {local_end})")
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] local_expert_mapping: {self.local_expert_mapping.tolist()}")
+                
+                # Count tokens per expert (global ids)
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] Token distribution (global expert ids):")
+                for expert_id in range(self.num_experts):
+                    count = (topk_ids_before == expert_id).sum().item()
+                    is_local = self.local_expert_mapping[expert_id].item() >= 0
+                    #logger.info(f"[standard.py]  Expert {expert_id}: {count} tokens, local={is_local}")
+                    
             if TopKOutputChecker.format_is_standard(topk_output):
                 topk_output = topk_output._replace(
                     topk_ids=self.local_expert_mapping[topk_output.topk_ids]
                 )
+                # Debug print after mapping
+                if should_print:
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output {topk_output}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output.topk_ids.shape: {topk_output.topk_ids.shape}, dtype: {topk_output.topk_ids.dtype}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output[0, :5].topk_ids: {topk_output.topk_ids[0, :5].tolist()}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output.weights.shape: {topk_output.topk_weights.shape}, dtype:{topk_output.topk_weights.dtype}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output[0, :5].weights: {topk_output.topk_weights[0, :5].tolist()}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] hidden_states.shape: {hidden_states.shape}, dtype: {hidden_states.dtype}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] hidden_states[0, :5]: {hidden_states[0, :5].tolist()}")
+                
             elif TopKOutputChecker.format_is_triton_kernels(topk_output):
                 raise NotImplementedError()
 
diff --git a/python/sglang/srt/models/qwen3_moe.py b/python/sglang/srt/models/qwen3_moe.py
index e277d46..8455ede 100644
--- a/python/sglang/srt/models/qwen3_moe.py
+++ b/python/sglang/srt/models/qwen3_moe.py
@@ -32,6 +32,7 @@ from sglang.srt.distributed import (
     get_tensor_model_parallel_rank,
     get_tensor_model_parallel_world_size,
     tensor_model_parallel_all_reduce,
+    tensor_model_parallel_tree_all_reduce,
 )
 from sglang.srt.eplb.expert_distribution import get_global_expert_distribution_recorder
 from sglang.srt.eplb.expert_location import ModelConfigForExpertLocation
@@ -75,7 +76,7 @@ from sglang.srt.utils import (
     is_non_idle_and_non_empty,
     is_npu,
 )
-
+import os
 _is_cuda = is_cuda()
 
 if _is_cuda:
@@ -94,6 +95,7 @@ _is_npu = is_npu()
 if _is_npu:
     from sgl_kernel_npu.norm.split_qkv_rmsnorm_rope import split_qkv_rmsnorm_rope
 
+logger.info("AAAAA Qwen3_moe.py")
 
 def compute_yarn_parameters(
     config: PretrainedConfig,
@@ -216,6 +218,7 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
         prefix: str = "",
     ):
         super().__init__()
+        self.use_fp32_route = get_global_server_args().enable_fp32_route
         self.tp_size = get_tensor_model_parallel_world_size()
         self.layer_id = layer_id
         if self.tp_size > config.num_experts:
@@ -244,13 +247,23 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
             routing_method_type=RoutingMethodType.Renormalize,
         )
 
-        self.gate = ReplicatedLinear(
-            config.hidden_size,
-            config.num_experts,
-            bias=False,
-            quant_config=None,
-            prefix=add_prefix("gate", prefix),
-        )
+        if self.use_fp32_route:
+            self.gate = ReplicatedLinear(
+                config.hidden_size,
+                config.num_experts,
+                bias=False,
+                quant_config=None,
+                prefix=add_prefix("gate", prefix),
+                params_dtype=torch.float32,
+            )
+        else:
+            self.gate = ReplicatedLinear(
+                config.hidden_size,
+                config.num_experts,
+                bias=False,
+                quant_config=None,
+                prefix=add_prefix("gate", prefix),
+            )
 
         if get_moe_a2a_backend().is_deepep():
             # TODO: we will support tp < ep in the future
@@ -295,8 +308,14 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
         hidden_states = hidden_states.view(-1, hidden_dim)
 
         # router_logits: (num_tokens, n_experts)
-        router_logits, _ = self.gate(hidden_states)
-
+        if self.use_fp32_route:
+            if num_tokens <= 2 and os.environ.get("DEBUG_ROUTER") == "1" and self.layer_id == 0:
+                logger.info("use fp32 route")
+            router_logits, _ = self.gate(hidden_states.to(torch.float32))
+        else:
+            router_logits, _ = self.gate(hidden_states)
+            if num_tokens <= 2 and os.environ.get("DEBUG_ROUTER") == "1" and self.layer_id == 0:
+                logger.info("use fp16 route")
         if get_global_server_args().rl_on_policy_target is not None:
             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)
             routing_weights, selected_experts = torch.topk(
@@ -309,6 +328,23 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
                 topk_ids=selected_experts,
                 router_logits=router_logits,
             )
+            #debug
+            if num_tokens <= 2 and os.environ.get("DEBUG_ROUTER") == "1" and self.layer_id == 0:
+                logger.info("rank %d", get_tensor_model_parallel_rank())
+                logger.info(f"Qwen3MoeSparseMoeBlock use_fp32_route: {self.use_fp32_route}")
+                logger.info(f"Qwen3MoeSparseMoeBlock num_tokensd: {num_tokens}")
+                logger.info(f"router_logits: {router_logits.shape}")
+                logger.info(f"Qwen3MoeSparseMoeBlock hidden_states: {hidden_states.shape}, hidden_states.dtype: {hidden_states.dtype}")
+                logger.info(f"Qwen3MoeSparseMoeBlock hidden_states[:2,:5]: {hidden_states[:2, :5].tolist()}")
+                logger.info(f"Qwen3MoeSparseMoeBlock router_logits[:2,:5]: {router_logits[:2, :5].tolist()}, router_logits.dtype: {router_logits.dtype}")
+                logger.info(f"Qwen3MoeSparseMoeBlock routing_weights: {routing_weights.shape}")
+                logger.info(f"Qwen3MoeSparseMoeBlock routing_weights[:2,:5]: {routing_weights[:2, :5].tolist()}, routing_weights.dtype: {routing_weights.dtype}")
+                logger.info(f"Qwen3MoeSparseMoeBlock selected_experts: {selected_experts.shape}")
+                logger.info(f"Qwen3MoeSparseMoeBlock selected_experts[:2,:5]: {selected_experts[:2, :5].tolist()}")
+                logger.info(f"Qwen3MoeSparseMoeBlock router_logits: {router_logits.shape}")
+                logger.info(f"Qwen3MoeSparseMoeBlock router_logits[:2,:5]: {router_logits[:2, :5].tolist()}")
+                logger.info(f"Qwen3MoeSparseMoeBlock topk_output: {topk_output.shape}")
+                logger.info(f"Qwen3MoeSparseMoeBlock topk_output[:2,:5]: {topk_output[:2, :5].tolist()}")
         else:
             topk_output = self.topk(hidden_states, router_logits)
 
@@ -319,7 +355,10 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
             and not use_reduce_scatter
             and not should_use_flashinfer_cutlass_moe_fp4_allgather()
         ):
-            final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
+            if get_global_server_args().rl_on_policy_target == "fsdp_tp":
+                final_hidden_states = tensor_model_parallel_tree_all_reduce(final_hidden_states)
+            else:
+                final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
 
         return final_hidden_states.view(num_tokens, hidden_dim)
 
@@ -781,7 +820,7 @@ class Qwen3MoeDecoderLayer(nn.Module):
             layer_scatter_modes=self.layer_scatter_modes,
             input_layernorm=self.input_layernorm,
             post_attention_layernorm=self.post_attention_layernorm,
-            allow_reduce_scatter=True,
+            allow_reduce_scatter=False,
             is_last_layer=(self.layer_id == self.config.num_hidden_layers - 1),
         )
 
@@ -939,6 +978,8 @@ class Qwen3MoeForCausalLM(nn.Module):
             prefix=add_prefix("lm_head", prefix),
             use_attn_tp_group=get_global_server_args().enable_dp_lm_head,
         )
+        if get_global_server_args().enable_fp32_lm_head:
+            self.lm_head = self.lm_head.float()
         self.logits_processor = LogitsProcessor(config)
         self.capture_aux_hidden_states = False
 
diff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py
index 72db298..c5aaa28 100644
--- a/python/sglang/srt/server_args.py
+++ b/python/sglang/srt/server_args.py
@@ -164,7 +164,7 @@ NSA_CHOICES = [
 
 RADIX_EVICTION_POLICY_CHOICES = ["lru", "lfu"]
 
-RL_ON_POLICY_TARGET_CHOICES = ["fsdp"]
+RL_ON_POLICY_TARGET_CHOICES = ["fsdp", "fsdp_tp"]
 
 MOE_RUNNER_BACKEND_CHOICES = [
     "auto",
@@ -285,6 +285,7 @@ class ServerArgs:
     quantization_param_path: Optional[str] = None
     kv_cache_dtype: str = "auto"
     enable_fp32_lm_head: bool = False
+    enable_fp32_route: bool = False
     modelopt_quant: Optional[Union[str, Dict]] = None
     modelopt_checkpoint_restore_path: Optional[str] = None
     modelopt_checkpoint_save_path: Optional[str] = None
@@ -2708,6 +2709,11 @@ class ServerArgs:
             action="store_true",
             help="If set, the LM head outputs (logits) are in FP32.",
         )
+        parser.add_argument(
+            "--enable-fp32-route",
+            action="store_true",
+            help="If set, the route(moe) are in FP32.",
+        )
         parser.add_argument(
             "--modelopt-quant",
             type=str,
