diff --git a/python/sglang/srt/distributed/communication_op.py b/python/sglang/srt/distributed/communication_op.py
index 95600ed..4d1b9c7 100644
--- a/python/sglang/srt/distributed/communication_op.py
+++ b/python/sglang/srt/distributed/communication_op.py
@@ -33,3 +33,9 @@ def broadcast_tensor_dict(
     if not torch.distributed.is_initialized():
         return tensor_dict
     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)
+
+def tensor_model_parallel_tree_all_reduce(input_: torch.Tensor) -> torch.Tensor:
+    """Tree all-reduce the input tensor across model parallel group."""
+    from sglang.srt.tp_invariant_ops import tree_all_reduce_sum
+
+    return tree_all_reduce_sum(input_, device_group=get_tp_group().device_group)
\ No newline at end of file
diff --git a/python/sglang/srt/layers/communicator.py b/python/sglang/srt/layers/communicator.py
index 15df851..56b664a 100644
--- a/python/sglang/srt/layers/communicator.py
+++ b/python/sglang/srt/layers/communicator.py
@@ -25,6 +25,7 @@ from sglang.srt.distributed import (
     get_tensor_model_parallel_world_size,
     get_tp_group,
     tensor_model_parallel_all_reduce,
+    tensor_model_parallel_tree_all_reduce, 
 )
 from sglang.srt.distributed.device_communicators.pynccl_allocator import (
     use_symmetric_memory,
@@ -38,6 +39,7 @@ from sglang.srt.layers.dp_attention import (
     attn_tp_reduce_scatter_tensor,
     dp_gather_partial,
     dp_reduce_scatter_tensor,
+    dp_gather_replicate,
     dp_scatter,
     get_attention_dp_size,
     get_attention_tp_rank,
@@ -806,7 +808,10 @@ class CommunicateWithAllReduceAndLayerNormFn:
                     hidden_states, residual
                 )
             else:
-                hidden_states = tensor_model_parallel_all_reduce(hidden_states)
+                if get_global_server_args().rl_on_policy_target == "fsdp_tp" or get_global_server_args().rl_on_policy_target == "fsdp":
+                    hidden_states = tensor_model_parallel_tree_all_reduce(hidden_states)
+                else:
+                    hidden_states = tensor_model_parallel_all_reduce(hidden_states)
                 if _is_npu and context.cache is not None:
                     _ = prepare_weight_cache(hidden_states, context.cache)
                 hidden_states, residual = layernorm(hidden_states, residual)
diff --git a/python/sglang/srt/layers/linear.py b/python/sglang/srt/layers/linear.py
index 428f3a2..88da0b2 100644
--- a/python/sglang/srt/layers/linear.py
+++ b/python/sglang/srt/layers/linear.py
@@ -1417,7 +1417,14 @@ class RowParallelLinear(LinearBase):
             output_parallel = self.quant_method.apply(self, input_parallel, bias=bias_)
 
         if self.reduce_results and self.tp_size > 1 and not skip_all_reduce:
-            output = tensor_model_parallel_all_reduce(output_parallel)
+            # Use tree_all_reduce for true on-policy mode to match Megatron
+            from sglang.srt.server_args import get_global_server_args
+            server_args = get_global_server_args()
+            if server_args is not None and server_args.rl_on_policy_target in ("fsdp", "fsdp_tp"):
+                from sglang.srt.distributed import tensor_model_parallel_tree_all_reduce
+                output = tensor_model_parallel_tree_all_reduce(output_parallel)
+            else:
+                output = tensor_model_parallel_all_reduce(output_parallel)
         else:
             output = output_parallel
 
diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
index 14d6923..d6bcc33 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
@@ -8,7 +8,8 @@ from __future__ import annotations
 import functools
 import os
 from typing import TYPE_CHECKING, List, Optional
-
+from sglang.srt.tp_invariant_ops.tp_invariant_ops import moe_sum_tree_reduce
+from sglang.srt.server_args import get_global_server_args
 import torch
 import torch.nn.functional as F
 import triton.language as tl
@@ -320,7 +321,21 @@ def fused_experts_impl(
     gemm1_alpha: Optional[float] = None,
     gemm1_limit: Optional[float] = None,
     filter_expert: bool = True,
+    layer_id: Optional[int] = None,  # Optional layer_id for debugging
 ):
+    # import logging
+    # import os
+    # logger = logging.getLogger(__name__)
+    # logger.info(f"fused_experts_impl")
+    # logger.info(f"hidden_states.shape: {hidden_states.shape}, dtype: {hidden_states.dtype}")
+    # if hidden_states.shape[0] > 91:
+    #     logger.info(f"hidden_states[91, :5]: {hidden_states[91, :5].tolist()}")
+    # else:
+    #     logger.info(f"hidden_states[0, :5]: {hidden_states[0, :5].tolist()}")
+    # logger.info(f"w1.shape: {w1.shape}, dtype: {w1.dtype}")
+    # logger.info(f"w2.shape: {w2.shape}, dtype: {w2.dtype}")
+    # logger.info(f"topk_weights.shape: {topk_weights.shape}, dtype: {topk_weights.dtype}")
+    # logger.info(f"topk_ids.shape: {topk_ids.shape}, dtype: {topk_ids.dtype}")
     padded_size = padding_size
     if not (use_fp8_w8a8 or use_int8_w8a8) or block_shape is not None or _use_aiter:
         padded_size = 0
@@ -560,7 +575,10 @@ def fused_experts_impl(
 
         if routed_scaling_factor is None:
             routed_scaling_factor = 1.0
-
+        # import logging
+        # logger = logging.getLogger(__name__)
+        # logger.info(f"fused MOE RL on policy target: {get_global_server_args().rl_on_policy_target}")
+                
         if no_combine:
             pass
         elif _is_cuda:
@@ -572,6 +590,14 @@ def fused_experts_impl(
                     intermediate_cache3[:, 1],
                     out=out_hidden_states[begin_chunk_idx:end_chunk_idx],
                 ).squeeze(dim=1)
+            elif get_global_server_args().rl_on_policy_target and get_global_server_args().rl_on_policy_target == "fsdp_tp" or get_global_server_args().rl_on_policy_target == "fsdp":
+                moe_sum_tree_reduce(
+                    intermediate_cache3.view(*intermediate_cache3.shape),
+                    out_hidden_states[begin_chunk_idx:end_chunk_idx],
+                    curr_topk_ids,
+                    routed_scaling_factor,
+                    E,
+                )
             else:
                 # According to micro benchmark results, torch.compile can get better performance for small token.
                 if (
diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
index 8394635..379783b 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
@@ -67,7 +67,7 @@ from sglang.srt.utils import (
     next_power_of_2,
     round_up,
 )
-from sglang.srt.utils.custom_op import register_custom_op
+from sglang.srt.utils.common import direct_register_custom_op
 
 if is_flashinfer_available():
     from flashinfer import fp4_quantize
@@ -909,7 +909,7 @@ class FusedMoE(torch.nn.Module):
             assert TopKOutputChecker.format_is_standard(
                 topk_output
             ), "Only standard topk output is supported for piecewise cuda graph"
-            return moe_forward_piecewise_cuda_graph_impl(
+            return torch.ops.sglang.moe_forward_piecewise_cuda_graph_impl(
                 hidden_states,
                 topk_output.topk_weights,
                 topk_output.topk_ids,
@@ -923,6 +923,18 @@ class FusedMoE(torch.nn.Module):
         origin_hidden_states_dim = hidden_states.shape[-1]
         assert self.quant_method is not None
 
+        # Debug: compare expert input (SGLang side) - only print for layer 0
+        import os
+        if os.environ.get("DEBUG_SGLANG_EP_MAPPING", "0") == "1" and self.layer_id == 0:
+            import logging
+            _logger = logging.getLogger(__name__)
+            position = 91 if hidden_states.shape[0] > 91 else 0
+            _logger.info(f"[layer.py][SGLang Expert Input][Layer {self.layer_id}] hidden_states.shape: {hidden_states.shape}, dtype: {hidden_states.dtype}")
+            _logger.info(f"[layer.py][SGLang Expert Input][Layer {self.layer_id}] hidden_states[{position}, :5]: {hidden_states[position, :5].tolist()}")
+            _logger.info(f"[layer.py][SGLang Expert Input][Layer {self.layer_id}] w13_weight.shape: {self.w13_weight.shape}, w2_weight.shape: {self.w2_weight.shape}")
+            _logger.info(f"[layer.py][SGLang Expert Input][Layer {self.layer_id}] topk_ids.shape: {topk_output.topk_ids.shape}, topk_ids[{position}]: {topk_output.topk_ids[position].tolist()}")
+            _logger.info(f"[layer.py][SGLang Expert Input][Layer {self.layer_id}] topk_weights.shape: {topk_output.topk_weights.shape}, topk_weights[{position}]: {topk_output.topk_weights[position].tolist()}")
+
         dispatch_output = self.dispatcher.dispatch(
             hidden_states=hidden_states, topk_output=topk_output
         )
@@ -953,6 +965,16 @@ class FusedMoE(torch.nn.Module):
         if self.reduce_results and (self.moe_tp_size > 1 or self.moe_ep_size > 1):
             final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
 
+        # Debug: compare expert output (SGLang side) - only print for layer 0
+        import os
+        if os.environ.get("DEBUG_SGLANG_EP_MAPPING", "0") == "1" and self.layer_id == 0:
+            import logging
+            _logger = logging.getLogger(__name__)
+            _logger.info(f"[layer.py][SGLang Expert Output][Layer {self.layer_id}] output.shape: {final_hidden_states.shape}, dtype: {final_hidden_states.dtype}")
+            _logger.info(f"[layer.py][SGLang Expert Output][Layer {self.layer_id}] output[0, :5]: {final_hidden_states[0, :5].tolist()}")
+            _logger.info(f"[layer.py][SGLang Expert Output][Layer {self.layer_id}] output.norm(): {final_hidden_states.norm().item():.6f}")
+            _logger.info(f"[layer.py][SGLang Expert Output][Layer {self.layer_id}] output.sum(): {final_hidden_states.sum().item():.6f}")
+
         return final_hidden_states
 
     def run_moe_core(self, dispatch_output: DispatchOutput) -> CombineInput:
@@ -1081,7 +1103,7 @@ class FlashInferFusedMoE(FusedMoE):
             assert TopKOutputChecker.format_is_standard(
                 topk_output
             ), "Only standard topk output is supported for piecewise cuda graph"
-            return moe_forward_piecewise_cuda_graph_impl(
+            return torch.ops.sglang.moe_forward_piecewise_cuda_graph_impl(
                 hidden_states,
                 topk_output.topk_weights,
                 topk_output.topk_ids,
@@ -1210,14 +1232,16 @@ class FlashInferFP4MoE(FusedMoE):
         ), "Only bypassed topk output is supported for flashinfer fp4 moe"
 
         if is_in_piecewise_cuda_graph():
-            return flashinfer_fp4_moe_forward_piecewise_cuda_graph_impl(
-                hidden_states,
-                topk_output.router_logits,
-                topk_output.topk_config.top_k,
-                topk_output.topk_config.topk_group,
-                topk_output.topk_config.num_expert_group,
-                topk_output.topk_config.correction_bias,
-                self.layer_id,
+            return (
+                torch.ops.sglang.flashinfer_fp4_moe_forward_piecewise_cuda_graph_impl(
+                    hidden_states,
+                    topk_output.router_logits,
+                    topk_output.topk_config.top_k,
+                    topk_output.topk_config.topk_group,
+                    topk_output.topk_config.num_expert_group,
+                    topk_output.topk_config.correction_bias,
+                    self.layer_id,
+                )
             )
         else:
             return self.forward_impl(hidden_states, topk_output)
@@ -1315,7 +1339,6 @@ class FlashInferFP4MoE(FusedMoE):
         return result
 
 
-@register_custom_op(out_shape="hidden_states")
 def moe_forward_piecewise_cuda_graph_impl(
     hidden_states: torch.Tensor,
     topk_weights: torch.Tensor,
@@ -1332,7 +1355,16 @@ def moe_forward_piecewise_cuda_graph_impl(
     return moe_layer.forward_impl(hidden_states, topk_output)
 
 
-@register_custom_op(out_shape="hidden_states")
+def moe_forward_piecewise_cuda_graph_impl_fake(
+    hidden_states: torch.Tensor,
+    topk_weights: torch.Tensor,
+    topk_ids: torch.Tensor,
+    router_logits: torch.Tensor,
+    layer_id: int,
+) -> torch.Tensor:
+    return torch.empty_like(hidden_states)
+
+
 def flashinfer_fp4_moe_forward_piecewise_cuda_graph_impl(
     hidden_states: torch.Tensor,
     router_logits: torch.Tensor,
@@ -1355,3 +1387,30 @@ def flashinfer_fp4_moe_forward_piecewise_cuda_graph_impl(
     forward_context = get_forward_context()
     moe_layer = forward_context.moe_layers[layer_id]
     return moe_layer.forward_impl(hidden_states, topk_output)
+
+
+def flashinfer_fp4_moe_forward_piecewise_cuda_graph_impl_fake(
+    hidden_states: torch.Tensor,
+    router_logits: torch.Tensor,
+    top_k: int,
+    topk_group: Optional[int],
+    num_expert_group: Optional[int],
+    correction_bias: Optional[torch.Tensor],
+    layer_id: int,
+) -> torch.Tensor:
+    return torch.empty_like(hidden_states)
+
+
+direct_register_custom_op(
+    op_name="moe_forward_piecewise_cuda_graph_impl",
+    op_func=moe_forward_piecewise_cuda_graph_impl,
+    mutates_args=[],
+    fake_impl=moe_forward_piecewise_cuda_graph_impl_fake,
+)
+
+direct_register_custom_op(
+    op_name="flashinfer_fp4_moe_forward_piecewise_cuda_graph_impl",
+    op_func=flashinfer_fp4_moe_forward_piecewise_cuda_graph_impl,
+    mutates_args=[],
+    fake_impl=flashinfer_fp4_moe_forward_piecewise_cuda_graph_impl_fake,
+)
diff --git a/python/sglang/srt/layers/moe/token_dispatcher/standard.py b/python/sglang/srt/layers/moe/token_dispatcher/standard.py
index 2c959c7..78ad6bb 100644
--- a/python/sglang/srt/layers/moe/token_dispatcher/standard.py
+++ b/python/sglang/srt/layers/moe/token_dispatcher/standard.py
@@ -32,6 +32,9 @@ from sglang.srt.layers.moe.utils import (
 )
 from sglang.srt.utils.common import get_bool_env_var, is_hip, is_sm120_supported
 
+import logging
+logger = logging.getLogger(__name__)
+
 _is_hip = is_hip()
 _use_aiter = get_bool_env_var("SGLANG_USE_AITER") and _is_hip
 
@@ -93,11 +96,12 @@ class StandardDispatcher(BaseDispatcher):
         )
         self.moe_ep_rank = get_moe_expert_parallel_rank()
         self.local_expert_mapping = None
+        self.layer_id = moe_runner_config.layer_id  # Store layer_id from config
 
     def dispatch(
         self, hidden_states: torch.Tensor, topk_output: TopKOutput
     ) -> StandardDispatchOutput:
-
+        import os
         if should_use_flashinfer_cutlass_moe_fp4_allgather():
             # all-gather fp4 hidden states
             from flashinfer import nvfp4_block_scale_interleave
@@ -167,10 +171,47 @@ class StandardDispatcher(BaseDispatcher):
                     )
 
         if self.local_expert_mapping is not None and not _use_aiter:
+            # Debug print for EP token mapping (only print for layer 0)
+            should_print = (
+                os.environ.get("DEBUG_SGLANG_EP_MAPPING", "0") == "1"
+                and self.layer_id == 0
+            )
+            
+            if should_print:
+                topk_ids_before = topk_output.topk_ids
+                num_tokens = topk_ids_before.shape[0]
+                topk = topk_ids_before.shape[1]
+                rank = self.moe_ep_rank
+                local_start = rank * self.num_local_routed_experts
+                local_end = local_start + self.num_local_routed_experts
+                
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] EP config: ep_size={self.moe_ep_size}, ep_rank={rank}")
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] num_experts={self.num_experts}, num_local_experts={self.num_local_routed_experts}")
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] num_tokens={num_tokens}, topk={topk}")
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] Local expert range: [{local_start}, {local_end})")
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] local_expert_mapping: {self.local_expert_mapping.tolist()}")
+                
+                # Count tokens per expert (global ids)
+                logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] Token distribution (global expert ids):")
+                for expert_id in range(self.num_experts):
+                    count = (topk_ids_before == expert_id).sum().item()
+                    is_local = self.local_expert_mapping[expert_id].item() >= 0
+                    #logger.info(f"[standard.py]  Expert {expert_id}: {count} tokens, local={is_local}")
+                    
             if TopKOutputChecker.format_is_standard(topk_output):
                 topk_output = topk_output._replace(
                     topk_ids=self.local_expert_mapping[topk_output.topk_ids]
                 )
+                # Debug print after mapping
+                if should_print:
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output {topk_output}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output.topk_ids.shape: {topk_output.topk_ids.shape}, dtype: {topk_output.topk_ids.dtype}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output[0, :5].topk_ids: {topk_output.topk_ids[0, :5].tolist()}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output.weights.shape: {topk_output.topk_weights.shape}, dtype:{topk_output.topk_weights.dtype}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] topk_output[0, :5].weights: {topk_output.topk_weights[0, :5].tolist()}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] hidden_states.shape: {hidden_states.shape}, dtype: {hidden_states.dtype}")
+                    logger.info(f"[standard.py][SGLang EP Mapping][Rank {rank}][Layer {self.layer_id}] hidden_states[0, :5]: {hidden_states[0, :5].tolist()}")
+                
             elif TopKOutputChecker.format_is_triton_kernels(topk_output):
                 raise NotImplementedError()
 
diff --git a/python/sglang/srt/models/qwen3.py b/python/sglang/srt/models/qwen3.py
index 47a1a4e..38102e2 100644
--- a/python/sglang/srt/models/qwen3.py
+++ b/python/sglang/srt/models/qwen3.py
@@ -139,8 +139,29 @@ class Qwen3Attention(nn.Module):
         self.alt_stream = alt_stream
 
     def forward_prepare_native(self, positions, hidden_states):
+        import os
+        layer_id = self.attn.layer_id
+        debug_attn = os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and layer_id <= 1
+        
         qkv, _ = self.qkv_proj(hidden_states)
         q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        
+        # DEBUG: QKV after projection (before qk_norm and RoPE)
+        if debug_attn:
+            tp_size = get_tensor_model_parallel_world_size()
+            tp_rank = get_tensor_model_parallel_rank()
+            attn_tp_size = get_attention_tp_size()
+            attn_tp_rank = get_attention_tp_rank()
+            pos = 0
+            prefix = f"[qwen3.py][SGLang][TP {tp_rank}/{tp_size}][AttnTP {attn_tp_rank}/{attn_tp_size}][Layer {layer_id}]"
+            logger.info(f"{prefix} QKV output (before qk_norm, before RoPE) q.shape: {q.shape}")
+            q_val = q[pos, :5] if q.dim() == 2 else q[pos, :5]
+            k_val = k[pos, :5] if k.dim() == 2 else k[pos, :5]
+            v_val = v[pos, :5] if v.dim() == 2 else v[pos, :5]
+            logger.info(f"{prefix} q (before qk_norm)[{pos},:5]: {q_val.tolist()}")
+            logger.info(f"{prefix} k (before qk_norm)[{pos},:5]: {k_val.tolist()}")
+            logger.info(f"{prefix} v[{pos},:5]: {v_val.tolist()}")
+        
         q, k = apply_qk_norm(
             q=q,
             k=k,
@@ -149,6 +170,12 @@ class Qwen3Attention(nn.Module):
             head_dim=self.head_dim,
             alt_stream=self.alt_stream,
         )
+        
+        # DEBUG: After qk_norm (before RoPE)
+        if debug_attn:
+            logger.info(f"{prefix} After qk_norm (before RoPE) q[{pos},:5]: {q[pos, :5].tolist()}")
+            logger.info(f"{prefix} After qk_norm (before RoPE) k[{pos},:5]: {k[pos, :5].tolist()}")
+        
         q, k = self.rotary_emb(positions, q, k)
         return q, k, v
 
@@ -178,6 +205,22 @@ class Qwen3Attention(nn.Module):
         hidden_states: torch.Tensor,
         forward_batch: ForwardBatch,
     ) -> torch.Tensor:
+        import os
+        layer_id = self.attn.layer_id
+        debug_attn = os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and layer_id <= 1
+        
+        if debug_attn:
+            tp_size = get_tensor_model_parallel_world_size()
+            tp_rank = get_tensor_model_parallel_rank()
+            attn_tp_size = get_attention_tp_size()
+            attn_tp_rank = get_attention_tp_rank()
+            pos = 0
+            prefix = f"[qwen3.py][SGLang][TP {tp_rank}/{tp_size}][AttnTP {attn_tp_rank}/{attn_tp_size}][Layer {layer_id}]"
+            # Attention input
+            logger.info(f"{prefix} Attention INPUT hidden_states.shape: {hidden_states.shape}")
+            logger.info(f"{prefix} Attention INPUT hidden_states[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+            logger.info(f"{prefix} Attention INPUT hidden_states sum: {hidden_states[pos].float().sum().item():.6f}")
+        
         if get_global_server_args().rl_on_policy_target is not None:
             hidden_states = hidden_states.bfloat16()
 
@@ -193,12 +236,41 @@ class Qwen3Attention(nn.Module):
                 forward_batch=forward_batch,
             )
 
+        # DEBUG: QKV after projection and RoPE
+        if debug_attn:
+            logger.info(f"{prefix} QKV output (after RoPE) q.shape: {q.shape}")
+            q_val = q[pos, :5] if q.dim() == 2 else (q[pos, 0, :5] if q.dim() == 3 else q[pos, :5])
+            k_val = k[pos, :5] if k.dim() == 2 else (k[pos, 0, :5] if k.dim() == 3 else k[pos, :5])
+            v_val = v[pos, :5] if v.dim() == 2 else (v[pos, 0, :5] if v.dim() == 3 else v[pos, :5])
+            logger.info(f"{prefix} q[{pos},:5]: {q_val.tolist()}")
+            logger.info(f"{prefix} k[{pos},:5]: {k_val.tolist()}")
+            logger.info(f"{prefix} v[{pos},:5]: {v_val.tolist()}")
+            logger.info(f"{prefix} After RoPE q norm: {q[pos].float().norm().item():.6f}")
+            logger.info(f"{prefix} After RoPE k norm: {k[pos].float().norm().item():.6f}")
+
         if get_global_server_args().rl_on_policy_target is not None:
             q = q.to(torch.bfloat16)
             k = k.to(torch.bfloat16)
 
         attn_output = self.attn(q, k, v, forward_batch)
+        
+        # DEBUG: Core attention output (before o_proj)
+        if debug_attn:
+            logger.info(f"{prefix} Core attention output shape: {attn_output.shape}")
+            attn_out_val = attn_output[pos, :5] if attn_output.dim() == 2 else attn_output[pos, :5]
+            logger.info(f"{prefix} Core attention output[{pos},:5]: {attn_out_val.tolist()}")
+            logger.info(f"{prefix} Core attention output norm: {attn_output[pos].float().norm().item():.6f}")
+            logger.info(f"{prefix} Core attention output sum: {attn_output[pos].float().sum().item():.6f}")
+        
         output, _ = self.o_proj(attn_output)
+        
+        # DEBUG: O_proj output (BEFORE all-reduce, since reduce_results=False)
+        if debug_attn:
+            logger.info(f"{prefix} o_proj output shape: {output.shape}")
+            output_val = output[pos, :5] if output.dim() == 2 else output[pos, :5]
+            logger.info(f"{prefix} o_proj output (BEFORE all-reduce)[{pos},:5]: {output_val.tolist()}")
+            logger.info(f"{prefix} o_proj output sum (BEFORE all-reduce): {output[pos].float().sum().item():.6f}")
+        
         return output
 
 
@@ -290,6 +362,39 @@ class Qwen3DecoderLayer(nn.Module):
                 forward_batch=forward_batch,
             )
 
+        # DEBUG: Before prepare_mlp (attention output, BEFORE all-reduce)
+        import os
+        layer_id = self.self_attn.attn.layer_id  # Get layer_id from attention module
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and layer_id <= 1:
+            from sglang.srt.layers.dp_attention import get_attention_tp_size, get_attention_tp_rank
+            from sglang.srt.tp_invariant_ops import tree_all_reduce_sum
+            from sglang.srt.distributed import get_tp_group
+            attn_tp_size = get_attention_tp_size()
+            attn_tp_rank = get_attention_tp_rank()
+            tp_size = get_tensor_model_parallel_world_size()
+            tp_rank = get_tensor_model_parallel_rank()
+            pos = 0
+            prefix = f"[qwen3.py][SGLang][TP {tp_rank}/{tp_size}][AttnTP {attn_tp_rank}/{attn_tp_size}][Layer {layer_id}]"
+            logger.info(f"{prefix} Before prepare_mlp (BEFORE all-reduce), hidden_states[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+            logger.info(f"{prefix} Before prepare_mlp (BEFORE all-reduce), hidden_states sum: {hidden_states[pos].float().sum().item():.6f}")
+            
+            # Manually do all-reduce for debug comparison with Megatron
+            # Use tree_all_reduce_sum to match what prepare_mlp actually uses in fsdp mode
+            if attn_tp_size > 1:
+                # Method 1: tree_all_reduce_sum (what SGLang actually uses in fsdp mode)
+                debug_tree_allreduce = tree_all_reduce_sum(hidden_states.clone(), device_group=get_tp_group().device_group)
+                logger.info(f"{prefix} DEBUG: After tree_all_reduce_sum (AFTER all-reduce), hidden_states[{pos},:5]: {debug_tree_allreduce[pos, :5].tolist()}")
+                logger.info(f"{prefix} DEBUG: After tree_all_reduce_sum (AFTER all-reduce), hidden_states sum: {debug_tree_allreduce[pos].float().sum().item():.6f}")
+                
+                # Method 2: NCCL all_reduce (what Megatron uses)
+                import torch.distributed as dist
+                debug_nccl_allreduce = hidden_states.clone()
+                dist.all_reduce(debug_nccl_allreduce, group=get_tp_group().device_group)
+                logger.info(f"{prefix} DEBUG: After NCCL all_reduce (Megatron style), hidden_states[{pos},:5]: {debug_nccl_allreduce[pos, :5].tolist()}")
+                logger.info(f"{prefix} DEBUG: After NCCL all_reduce (Megatron style), hidden_states sum: {debug_nccl_allreduce[pos].float().sum().item():.6f}")
+            else:
+                logger.info(f"{prefix} DEBUG: attn_tp_size=1, no all-reduce needed")
+
         # Fully Connected
         hidden_states, residual = self.layer_communicator.prepare_mlp(
             hidden_states,
@@ -301,12 +406,33 @@ class Qwen3DecoderLayer(nn.Module):
                 else None
             ),
         )
+        
+        # DEBUG: After prepare_mlp (after all-reduce + LayerNorm) - this is MLP input
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and layer_id <= 1:
+            logger.info(f"{prefix} After prepare_mlp (all-reduce + LN) = MLP INPUT, hidden_states[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+            logger.info(f"{prefix} After prepare_mlp = MLP INPUT, hidden_states sum: {hidden_states[pos].float().sum().item():.6f}")
+            if residual is not None:
+                logger.info(f"{prefix} After prepare_mlp, residual[{pos},:5]: {residual[pos, :5].tolist()}")
+                logger.info(f"{prefix} After prepare_mlp, residual sum: {residual[pos].float().sum().item():.6f}")
+
         hidden_states = self.mlp(hidden_states)
+        
+        # DEBUG: After MLP (before postprocess_layer) - MLP output includes all-reduce from down_proj
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and layer_id <= 1:
+            logger.info(f"{prefix} After MLP (before postprocess) = MLP OUTPUT, hidden_states[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+            logger.info(f"{prefix} After MLP = MLP OUTPUT, hidden_states sum: {hidden_states[pos].float().sum().item():.6f}")
+        
         if _is_npu and get_cmo_stream():
             wait_cmo_stream()
         hidden_states, residual = self.layer_communicator.postprocess_layer(
             hidden_states, residual, forward_batch
         )
+        
+        # DEBUG: After postprocess_layer - final layer output
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and layer_id <= 1:
+            logger.info(f"{prefix} After postprocess_layer = LAYER OUTPUT, hidden_states[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+            logger.info(f"{prefix} After postprocess_layer = LAYER OUTPUT, hidden_states sum: {hidden_states[pos].float().sum().item():.6f}")
+        
         return hidden_states, residual
 
 
diff --git a/python/sglang/srt/models/qwen3_moe.py b/python/sglang/srt/models/qwen3_moe.py
index e277d46..87040ff 100644
--- a/python/sglang/srt/models/qwen3_moe.py
+++ b/python/sglang/srt/models/qwen3_moe.py
@@ -32,6 +32,7 @@ from sglang.srt.distributed import (
     get_tensor_model_parallel_rank,
     get_tensor_model_parallel_world_size,
     tensor_model_parallel_all_reduce,
+    tensor_model_parallel_tree_all_reduce,
 )
 from sglang.srt.eplb.expert_distribution import get_global_expert_distribution_recorder
 from sglang.srt.eplb.expert_location import ModelConfigForExpertLocation
@@ -75,7 +76,7 @@ from sglang.srt.utils import (
     is_non_idle_and_non_empty,
     is_npu,
 )
-
+import os
 _is_cuda = is_cuda()
 
 if _is_cuda:
@@ -94,6 +95,7 @@ _is_npu = is_npu()
 if _is_npu:
     from sgl_kernel_npu.norm.split_qkv_rmsnorm_rope import split_qkv_rmsnorm_rope
 
+logger.info("AAAAA Qwen3_moe.py")
 
 def compute_yarn_parameters(
     config: PretrainedConfig,
@@ -216,6 +218,7 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
         prefix: str = "",
     ):
         super().__init__()
+        self.use_fp32_route = get_global_server_args().enable_fp32_route
         self.tp_size = get_tensor_model_parallel_world_size()
         self.layer_id = layer_id
         if self.tp_size > config.num_experts:
@@ -244,13 +247,23 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
             routing_method_type=RoutingMethodType.Renormalize,
         )
 
-        self.gate = ReplicatedLinear(
-            config.hidden_size,
-            config.num_experts,
-            bias=False,
-            quant_config=None,
-            prefix=add_prefix("gate", prefix),
-        )
+        if self.use_fp32_route:
+            self.gate = ReplicatedLinear(
+                config.hidden_size,
+                config.num_experts,
+                bias=False,
+                quant_config=None,
+                prefix=add_prefix("gate", prefix),
+                params_dtype=torch.float32,
+            )
+        else:
+            self.gate = ReplicatedLinear(
+                config.hidden_size,
+                config.num_experts,
+                bias=False,
+                quant_config=None,
+                prefix=add_prefix("gate", prefix),
+            )
 
         if get_moe_a2a_backend().is_deepep():
             # TODO: we will support tp < ep in the future
@@ -295,8 +308,14 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
         hidden_states = hidden_states.view(-1, hidden_dim)
 
         # router_logits: (num_tokens, n_experts)
-        router_logits, _ = self.gate(hidden_states)
-
+        if self.use_fp32_route:
+            if num_tokens <= 2 and os.environ.get("DEBUG_ROUTER") == "1" and self.layer_id == 0:
+                logger.info("use fp32 route")
+            router_logits, _ = self.gate(hidden_states.to(torch.float32))
+        else:
+            router_logits, _ = self.gate(hidden_states)
+            if num_tokens <= 2 and os.environ.get("DEBUG_ROUTER") == "1" and self.layer_id == 0:
+                logger.info("use fp16 route")
         if get_global_server_args().rl_on_policy_target is not None:
             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)
             routing_weights, selected_experts = torch.topk(
@@ -309,6 +328,31 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
                 topk_ids=selected_experts,
                 router_logits=router_logits,
             )
+            # debug
+            if num_tokens <= 2 and os.environ.get("DEBUG_ROUTER") == "1" and self.layer_id == 0:
+                rank = get_tensor_model_parallel_rank()
+                layer_id = self.layer_id
+                pos = 0
+                prefix = f"[qwen3_moe.py][SGLang Router][Rank {rank}][Layer {layer_id}]"
+                logger.info(f"{prefix} Shapes: "
+                            f"hidden_states={hidden_states.shape}, "
+                            f"router_logits={router_logits.shape}, "
+                            f"routing_weights={routing_weights.shape}, "
+                            f"selected_experts={selected_experts.shape}, "
+                            f"use_fp32_route={self.use_fp32_route}")
+                logger.info(f"{prefix} Hidden states[{pos},:5]: "
+                            f"{hidden_states[pos, :5].tolist()}, "
+                            f"hidden_states dtype: {hidden_states.dtype}")
+                logger.info(f"{prefix} Router logits[{pos},:5]: "
+                            f"{router_logits[pos, :5].tolist()}, "
+                            f"router_logits dtype: {router_logits.dtype}")
+                logger.info(f"{prefix} Routing weights[{pos},:] (after softmax+topk+renorm): "
+                            f"{routing_weights[pos, :].tolist()}, "
+                            f"routing_weights dtype: {routing_weights.dtype}")
+                logger.info(f"{prefix} Selected experts[{pos},:]: "
+                            f"{selected_experts[pos, :].tolist()}")
+                logger.info(f"{prefix} TopK weights[{pos},:]: "
+                            f"{topk_output.topk_weights[pos, :].tolist()}")
         else:
             topk_output = self.topk(hidden_states, router_logits)
 
@@ -319,7 +363,10 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
             and not use_reduce_scatter
             and not should_use_flashinfer_cutlass_moe_fp4_allgather()
         ):
-            final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
+            if get_global_server_args().rl_on_policy_target == "fsdp_tp":
+                final_hidden_states = tensor_model_parallel_tree_all_reduce(final_hidden_states)
+            else:
+                final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
 
         return final_hidden_states.view(num_tokens, hidden_dim)
 
@@ -568,9 +615,38 @@ class Qwen3MoeAttention(nn.Module):
         hidden_states: torch.Tensor,
         forward_batch: ForwardBatch,
     ):
+        # DEBUG: Attention input (after input_layernorm in decoder layer)
+        import os
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and self.attn.layer_id == 0:
+            tp_rank = get_tensor_model_parallel_rank()
+            pos = 0
+            prefix = f"[qwen3_moe.py][SGLang][TP {tp_rank}][Layer {self.attn.layer_id}]"
+            logger.info(f"{prefix} Attention input[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+            logger.info(f"{prefix} Attention input norm: {hidden_states[pos].float().norm().item():.6f}")
+        
         qkv, _ = self.qkv_proj(hidden_states)
+        
+        # DEBUG: QKV output (before RoPE)
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and self.attn.layer_id == 0:
+            q_raw, k_raw, v_raw = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+            q_reshaped = q_raw.view(-1, self.num_heads, self.head_dim)
+            k_reshaped = k_raw.view(-1, self.num_kv_heads, self.head_dim)
+            v_reshaped = v_raw.view(-1, self.num_kv_heads, self.head_dim)
+            logger.info(f"{prefix} QKV output (before RoPE) query.shape: {q_reshaped.shape}")
+            logger.info(f"{prefix} query[{pos},0,:5]: {q_reshaped[pos, 0, :5].tolist()}")
+            logger.info(f"{prefix} key[{pos},0,:5]: {k_reshaped[pos, 0, :5].tolist()}")
+            logger.info(f"{prefix} value[{pos},0,:5]: {v_reshaped[pos, 0, :5].tolist()}")
 
         q, k, v = self.apply_qk_norm_rope(qkv, positions, forward_batch)
+        
+        # DEBUG: Q, K after RoPE
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and self.attn.layer_id == 0:
+            q_reshaped = q.view(-1, self.num_heads, self.head_dim)
+            k_reshaped = k.view(-1, self.num_kv_heads, self.head_dim)
+            logger.info(f"{prefix} After RoPE query[{pos},0,:5]: {q_reshaped[pos, 0, :5].tolist()}")
+            logger.info(f"{prefix} After RoPE key[{pos},0,:5]: {k_reshaped[pos, 0, :5].tolist()}")
+            logger.info(f"{prefix} After RoPE query norm: {q[pos].float().norm().item():.6f}")
+            logger.info(f"{prefix} After RoPE key norm: {k[pos].float().norm().item():.6f}")
 
         inner_state = q, k, v, forward_batch
         return None, forward_batch, inner_state
@@ -671,7 +747,38 @@ class Qwen3MoeAttention(nn.Module):
             fb,
             save_kv_cache=save_kv_cache,
         )
+        
+        # DEBUG: Core attention output (before output projection)
+        import os
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and self.attn.layer_id == 0:
+            tp_rank = get_tensor_model_parallel_rank()
+            tp_size = get_tensor_model_parallel_world_size()
+            from sglang.srt.layers.dp_attention import get_attention_tp_rank, get_attention_tp_size
+            attn_tp_rank = get_attention_tp_rank()
+            attn_tp_size = get_attention_tp_size()
+            pos = 0  # Match Megatron's debug position
+            prefix = f"[qwen3_moe.py][SGLang][TP {tp_rank}/{tp_size}][AttnTP {attn_tp_rank}/{attn_tp_size}][Layer {self.attn.layer_id}]"
+            logger.info(f"{prefix} Core attention output shape: {attn_output.shape}")
+            logger.info(f"{prefix} Core attention output[{pos},:5]: {attn_output[pos, :5].tolist()}")
+            logger.info(f"{prefix} Core attention output norm: {attn_output[pos].float().norm().item():.6f}")
+            
+            # Debug o_proj weight with sum
+            weight = self.o_proj.weight
+            logger.info(f"{prefix} o_proj weight shape: {weight.shape}")
+            logger.info(f"{prefix} o_proj weight[0,:5]: {weight[0, :5].tolist()}")
+            logger.info(f"{prefix} o_proj weight sum: {weight.float().sum().item():.6f}")
+            
+            # Debug o_proj input with sum
+            logger.info(f"{prefix} o_proj input[{pos},:5]: {attn_output[pos, :5].tolist()}")
+            logger.info(f"{prefix} o_proj input sum: {attn_output[pos].float().sum().item():.6f}")
+        
         output, _ = self.o_proj(attn_output)
+        
+        # DEBUG: After o_proj (attention output) - before all-reduce
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and self.attn.layer_id == 0:
+            logger.info(f"{prefix} Attention output (after o_proj, BEFORE all-reduce)[{pos},:5]: {output[pos, :5].tolist()}")
+            logger.info(f"{prefix} Attention output sum (BEFORE all-reduce): {output[pos].float().sum().item():.6f}")
+        
         return output
 
     def forward(
@@ -781,7 +888,7 @@ class Qwen3MoeDecoderLayer(nn.Module):
             layer_scatter_modes=self.layer_scatter_modes,
             input_layernorm=self.input_layernorm,
             post_attention_layernorm=self.post_attention_layernorm,
-            allow_reduce_scatter=True,
+            allow_reduce_scatter=False,
             is_last_layer=(self.layer_id == self.config.num_hidden_layers - 1),
         )
 
@@ -812,9 +919,57 @@ class Qwen3MoeDecoderLayer(nn.Module):
                 forward_batch=forward_batch,
             )
 
+        # DEBUG: Attention output (before post_attention_layernorm)
+        import os
+        if os.environ.get("SLIME_DEBUG_ROUTER", "0") == "1" and self.layer_id == 0:
+            rank = get_tensor_model_parallel_rank()
+            pos = 0
+            prefix = f"[qwen3_moe.py][SGLang Decoder][Rank {rank}][Layer {self.layer_id}]"
+            logger.info(f"{prefix} Attention output (before post_attn_norm)[{pos},:5]: "
+                        f"{hidden_states[pos, :5].tolist()}, dtype: {hidden_states.dtype}")
+            if residual is not None:
+                logger.info(f"{prefix} Residual[{pos},:5]: {residual[pos, :5].tolist()}")
+
+        # DEBUG: Check attn_tp_size before prepare_mlp (all-reduce)
+        import os
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and self.layer_id == 0:
+            from sglang.srt.layers.dp_attention import get_attention_tp_size, get_attention_tp_rank, get_attention_tp_group
+            attn_tp_size = get_attention_tp_size()
+            attn_tp_rank = get_attention_tp_rank()
+            tp_size = get_tensor_model_parallel_world_size()
+            tp_rank = get_tensor_model_parallel_rank()
+            pos = 0
+            prefix = f"[qwen3_moe.py][SGLang][TP {tp_rank}/{tp_size}][AttnTP {attn_tp_rank}/{attn_tp_size}]"
+            logger.info(f"{prefix} Before prepare_mlp (BEFORE all-reduce), hidden_states[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+            logger.info(f"{prefix} Before prepare_mlp (BEFORE all-reduce), hidden_states sum: {hidden_states[pos].float().sum().item():.6f}")
+            
+            # Manually do all-reduce for debug comparison with Megatron
+            # This should match Megatron's "Attention output (AFTER all-reduce)"
+            if attn_tp_size > 1:
+                debug_after_allreduce = get_attention_tp_group().all_reduce(hidden_states.clone())
+                logger.info(f"{prefix} DEBUG: After manual attn_tp_all_reduce (AFTER all-reduce), hidden_states[{pos},:5]: {debug_after_allreduce[pos, :5].tolist()}")
+                logger.info(f"{prefix} DEBUG: After manual attn_tp_all_reduce (AFTER all-reduce), hidden_states sum: {debug_after_allreduce[pos].float().sum().item():.6f}")
+            else:
+                logger.info(f"{prefix} DEBUG: attn_tp_size=1, no all-reduce needed (AFTER all-reduce), hidden_states[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+                logger.info(f"{prefix} DEBUG: attn_tp_size=1, no all-reduce needed (AFTER all-reduce), hidden_states sum: {hidden_states[pos].float().sum().item():.6f}")
+        
         hidden_states, residual = self.layer_communicator.prepare_mlp(
             hidden_states, residual, forward_batch
         )
+        
+        # DEBUG: After prepare_mlp (all-reduce + LayerNorm, or just LayerNorm if attn_tp_size==1)
+        if os.environ.get("SLIME_DEBUG_ATTN", "0") == "1" and self.layer_id == 0:
+            if attn_tp_size > 1:
+                logger.info(f"{prefix} After prepare_mlp (all-reduce + LN), hidden_states[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+                logger.info(f"{prefix} After prepare_mlp (all-reduce + LN), hidden_states sum: {hidden_states[pos].float().sum().item():.6f}")
+            else:
+                logger.info(f"{prefix} After prepare_mlp (NO all-reduce, just LN), hidden_states[{pos},:5]: {hidden_states[pos, :5].tolist()}")
+                logger.info(f"{prefix} After prepare_mlp (NO all-reduce, just LN), hidden_states sum: {hidden_states[pos].float().sum().item():.6f}")
+
+        # DEBUG: After post_attention_layernorm (MoE input)
+        if os.environ.get("SLIME_DEBUG_ROUTER", "0") == "1" and self.layer_id == 0:
+            logger.info(f"{prefix} After post_attn_norm (MoE input)[{pos},:5]: "
+                        f"{hidden_states[pos, :5].tolist()}, dtype: {hidden_states.dtype}")
 
         should_allreduce_fusion = (
             self.layer_communicator.should_fuse_mlp_allreduce_with_next_layer(
@@ -939,6 +1094,8 @@ class Qwen3MoeForCausalLM(nn.Module):
             prefix=add_prefix("lm_head", prefix),
             use_attn_tp_group=get_global_server_args().enable_dp_lm_head,
         )
+        if get_global_server_args().enable_fp32_lm_head:
+            self.lm_head = self.lm_head.float()
         self.logits_processor = LogitsProcessor(config)
         self.capture_aux_hidden_states = False
 
diff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py
index 72db298..c5aaa28 100644
--- a/python/sglang/srt/server_args.py
+++ b/python/sglang/srt/server_args.py
@@ -164,7 +164,7 @@ NSA_CHOICES = [
 
 RADIX_EVICTION_POLICY_CHOICES = ["lru", "lfu"]
 
-RL_ON_POLICY_TARGET_CHOICES = ["fsdp"]
+RL_ON_POLICY_TARGET_CHOICES = ["fsdp", "fsdp_tp"]
 
 MOE_RUNNER_BACKEND_CHOICES = [
     "auto",
@@ -285,6 +285,7 @@ class ServerArgs:
     quantization_param_path: Optional[str] = None
     kv_cache_dtype: str = "auto"
     enable_fp32_lm_head: bool = False
+    enable_fp32_route: bool = False
     modelopt_quant: Optional[Union[str, Dict]] = None
     modelopt_checkpoint_restore_path: Optional[str] = None
     modelopt_checkpoint_save_path: Optional[str] = None
@@ -2708,6 +2709,11 @@ class ServerArgs:
             action="store_true",
             help="If set, the LM head outputs (logits) are in FP32.",
         )
+        parser.add_argument(
+            "--enable-fp32-route",
+            action="store_true",
+            help="If set, the route(moe) are in FP32.",
+        )
         parser.add_argument(
             "--modelopt-quant",
             type=str,
